import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# 使用 CIFAR-10 数据集作为示例
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()


# 选择两个类别用于二分类（如类别 0 和类别 1）
selected_classes = [0, 1]
mask_train = np.isin(y_train, selected_classes).flatten()
mask_test = np.isin(y_test, selected_classes).flatten()


x_train, y_train = x_train[mask_train], y_train[mask_train]
x_test, y_test = x_test[mask_test], y_test[mask_test]


# 正则化像素值
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0


# 转换标签为二分类
y_train = (y_train == selected_classes[1]).astype("int").flatten()
y_test = (y_test == selected_classes[1]).astype("int").flatten()


# 分离有标签数据和无标签数据
x_labeled, x_unlabeled, y_labeled, _ = train_test_split(x_train, y_train, test_size=0.8, random_state=42)


# 修正目标值形状
y_labeled = np.expand_dims(y_labeled, axis=-1)
y_test = np.expand_dims(y_test, axis=-1)


# 定义模型
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization


# 构建模型
def build_student_model(input_shape):
    inputs = Input(shape=input_shape)
    x = Conv2D(32, kernel_size=3, activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=2)(x)
    x = Conv2D(64, kernel_size=3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=2)(x)
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(1, activation='sigmoid')(x)  # 输出形状为 (batch_size, 1)
    return Model(inputs, outputs)


# 检查输出
student_model = build_student_model(input_shape=(32, 32, 3))
print(student_model.summary())


#student_model = build_student_model(input_shape=(32, 32, 3))
teacher_model = tf.keras.models.clone_model(student_model)  # 教师模型初始化为学生模型


# 定义损失函数
def consistency_loss(student_output, teacher_output):
    """
    一致性损失，鼓励学生模型和教师模型的输出相似。
    """
    return tf.reduce_mean(tf.square(student_output - teacher_output))

def supervised_loss(y_true, y_pred):
    """
    有标签数据的监督损失（交叉熵）。
    """
    return tf.keras.losses.binary_crossentropy(y_true, y_pred)


# Mean Teacher 训练流程
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
alpha = 0.99  # Teacher EMA 更新系数

@tf.function
def train_step(x_labeled_batch, y_labeled_batch, x_unlabeled_batch):
    with tf.GradientTape() as tape:
        # 学生模型前向传播
        student_labeled_output = student_model(x_labeled_batch, training=True)
        student_unlabeled_output = student_model(x_unlabeled_batch, training=True)

        # 教师模型前向传播（无梯度更新）
        teacher_unlabeled_output = teacher_model(x_unlabeled_batch, training=False)

        # 损失计算
        sup_loss = supervised_loss(y_labeled_batch, student_labeled_output)
        cons_loss = consistency_loss(student_unlabeled_output, teacher_unlabeled_output)
        total_loss = sup_loss + cons_loss

    # 学生模型反向传播
    gradients = tape.gradient(total_loss, student_model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))

    # 教师模型更新 (EMA)
    for t_weights, s_weights in zip(teacher_model.weights, student_model.weights):
        t_weights.assign(alpha * t_weights + (1 - alpha) * s_weights)

    return total_loss


# 模型训练
batch_size = 32
epochs = 10

# 数据分批次
labeled_dataset = tf.data.Dataset.from_tensor_slices((x_labeled, y_labeled)).batch(batch_size)
unlabeled_dataset = tf.data.Dataset.from_tensor_slices(x_unlabeled).batch(batch_size)


print(f"x_labeled_batch shape: {x_labeled_batch.shape}")
print(f"x_unlabeled_batch shape: {x_unlabeled_batch.shape}")





for epoch in range(epochs):
    total_loss = 0.0
    for (labeled_batch, unlabeled_batch) in zip(labeled_dataset, unlabeled_dataset):
        x_labeled_batch, y_labeled_batch = labeled_batch
        x_unlabeled_batch = unlabeled_batch
        loss = train_step(x_labeled_batch, y_labeled_batch, x_unlabeled_batch)
        total_loss += loss
    print(f"Epoch {epoch + 1}, Loss: {total_loss.numpy():.4f}")



